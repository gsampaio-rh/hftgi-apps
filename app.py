# Imports
from langchain.embeddings.huggingface import HuggingFaceEmbeddings
from langchain.llms import HuggingFaceTextGenInference
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from kafka import KafkaConsumer, KafkaProducer
import json
import logging
import uuid

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)

# Constants for the HF Inference server setup
INFERENCE_SERVER_URL = "http://localhost:3000/"

# Constants for the Kafka server setup
KAFKA_SERVER = "localhost:9092"
CONSUMER_TOPIC = "chat"
PRODUCER_TOPIC = "answer"

# Prompt Templates
template = """
        Given the conversation below, extract key information in a structured and concise manner. The goal is to parse out identifiable details such as personal names, email addresses, phone numbers, and any specific concerns or requests mentioned. This extraction should culminate in a structured JSON output that includes the following fields:

        - **Name**: The name(s) of the person(s) involved.
        - **Email**: Email address(es) mentioned.
        - **Phone Number**: Phone number(s) provided.
        - **Location**: Any specific locations mentioned in relation to the issue or service.
        - **Department**: The department or organization related if specified.
        - **Issue**: Brief description of the main issue(s) discussed.
        - **Service**: Specific service(s) mentioned in connection with the issue.
        - **Additional Information**: Any other relevant details or stakeholders mentioned.
        - **Detailed Description**: A comprehensive summary of the complaint or request, including any specific outcomes desired.

        The response should adhere to privacy and ethical guidelines, simplifying the information while preserving its original context and meaning. Assumptions beyond the provided data should be avoided.

        Conversation Transcript:
        {conversation}

        Note: Ensure your response is concise, avoiding repetition. If similar points are made more than once, summarize them in a single statement, focusing on providing a clear and structured summary of the conversation's key details.
        """

# Kafka Consumer Setup
def create_kafka_consumer(server, topic, group_id):
    """Initializes and returns a Kafka Consumer."""
    return KafkaConsumer(
        topic,
        bootstrap_servers=[server],
        auto_offset_reset="earliest",
        enable_auto_commit=True,
        group_id=group_id,
        value_deserializer=lambda x: json.loads(x.decode("utf-8")),
    )

# Kafka Producer Setup
def create_kafka_producer(server):
    """Initializes and returns a Kafka Producer."""
    return KafkaProducer(
        bootstrap_servers=[server],
        value_serializer=lambda x: json.dumps(x).encode("utf-8"),
    )

def convert_to_json(output):
    """Converts structured text to JSON format."""
    structured_text = output.get("text", "")
    keys = ["Name", "Email", "Phone Number", "Department", "Issue", "Service", "Additional Information", "Detailed Description"]
    data_dict = {}
    for line in structured_text.split("\n"):
        for key in keys:
            if line.strip().startswith(f"- **{key}**"):
                value = line.split(f"- **{key}**:")[1].strip()
                if value.lower() in ["not available", "não disponível"]:
                    value = None
                data_dict[key.replace(" ", "_").lower()] = value
    return json.dumps(data_dict, indent=4, ensure_ascii=False)

# Initialize LLMs and Chains
def setup_llm_chains(inference_url):
    """
    Initializes and configures the Large Language Model (LLM) Chains with the given inference server URL.
    
    This setup prepares an LLM for text generation by specifying behavior-modifying parameters to fine-tune
    the responses generated by the model. It encapsulates the model within an LLMChain, ready for executing
    text processing tasks using a predefined prompt template.
    
    Parameters:
    - inference_url (str): URL of the inference server where the LLM is hosted, responsible for performing
      the text generation tasks.
    
    Returns:
    - LLMChain: An instance of LLMChain, combining a prompt template with the LLM for text processing.
    
    The HuggingFaceTextGenInference model is initialized with the following parameters:
    - max_new_tokens (int): The maximum number of new tokens to generate. Set to 512, this limits the length
      of the generated response, controlling output verbosity.
    - top_k (int): Filters the generated predictions to the top-k probabilities before applying softmax.
      Set to 10, it focuses model choices, reducing the randomness of the response.
    - top_p (float): Nucleus sampling parameter controlling the cumulative probability cutoff. Set to 0.95,
      it allows for more diverse responses by only considering the top 95% probable options.
    - typical_p (float): Used to dynamically adjust top_p based on token probability distribution, aiming to
      maintain a typical set of options. Set to 0.95, it works in tandem with top_p for dynamic adjustments.
    - temperature (float): Controls the randomness of the output by scaling the logits before applying softmax.
      Set to 0.1, it produces more deterministic output, favoring higher probability options.
    - repetition_penalty (float): Increases/decreases the likelihood of previously generated tokens. Set to 1.175,
      it slightly penalizes repetition, encouraging more varied outputs.
    
    These parameters are carefully chosen to balance creativity, coherence, and control in the generated text,
    making the LLM more suitable for structured information extraction and analysis tasks.
    """
    qa_chain_prompt = PromptTemplate.from_template(template)
    
    llm = HuggingFaceTextGenInference(
        inference_server_url=inference_url,
        max_new_tokens=512,
        top_k=10,
        top_p=0.95,
        typical_p=0.95,
        temperature=0.1,
        repetition_penalty=1.175,
    )
    llm_chain = LLMChain(prompt=qa_chain_prompt, llm=llm)
    return llm_chain

def main():
    consumer = create_kafka_consumer(KAFKA_SERVER, CONSUMER_TOPIC, "chat-group")
    producer = create_kafka_producer(KAFKA_SERVER)
    llm_chain = setup_llm_chains(INFERENCE_SERVER_URL)

    for message in consumer:
        try:
            conversation_text = message.value['conversation']
            conversation_id = str(uuid.uuid4())
            logging.info(f"Processing conversation ID: {conversation_id}")

            # Process the conversation
            response = llm_chain.invoke({"conversation": conversation_text})
            json_response = convert_to_json(response)

            # Prepare and send the response
            result = {
                "id": conversation_id,
                "conversation": conversation_text,
                "json_response": json.loads(json_response)
            }
            producer.send(PRODUCER_TOPIC, value=result)
            logging.info("Processed and sent conversation to 'answer' topic.")

        except Exception as e:
            logging.error(f"Error processing message: {e}", exc_info=True)

if __name__ == "__main__":
    main()